{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search using Azure Cosmos DB for NoSQL\n",
    "\n",
    "This notebook demonstrates using an Azure OpenAI embedding model to vectorize documents already stored in Azure Cosmos DB for NoSQL API, storing the embedding vectors and the creation of a vector index. Lastly, the notebook will demonstrate how to query the vector index to find similar documents.\n",
    "\n",
    "This lab expects the data that was loaded in Lab 2. A current limitation is that the vector search feature for Azure Cosmos DB for NoSQL is supported only on new containers so the vector policy needs to be applied during the time of container creation and it canâ€™t be modified later, as such a new container `product_v` for products will be created in this notebook for use in this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from models import Product\n",
    "from pydantic import BaseModel\n",
    "from typing import Type, TypeVar, List\n",
    "from azure.cosmos import CosmosClient, DatabaseProxy, ContainerProxy, PartitionKey\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load settings\n",
    "\n",
    "This lab expects the `.env` file that was created in Lab 1 to obtain the connection string for the database.\n",
    "\n",
    "Add the following entries into the `.env` file to support the connection to Azure OpenAI API, replacing the values for `<your key>` and `<your endpoint>` with the values from your Azure OpenAI API resource.\n",
    "\n",
    "```text\n",
    "AOAI_ENDPOINT=\"<your endpoint>\"\n",
    "AOAI_KEY=\"<your key>\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "CONNECTION_STRING = os.environ.get(\"COSMOS_DB_CONNECTION_STRING\")\n",
    "EMBEDDINGS_DEPLOYMENT_NAME = \"embeddings\"\n",
    "COMPLETIONS_DEPLOYMENT_NAME = \"completions\"\n",
    "AOAI_ENDPOINT = os.environ.get(\"AOAI_ENDPOINT\")\n",
    "AOAI_KEY = os.environ.get(\"AOAI_KEY\")\n",
    "AOAI_API_VERSION = \"2024-06-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish connectivity to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Cosmos DB client\n",
    "client = CosmosClient.from_connection_string(CONNECTION_STRING)\n",
    "\n",
    "# Create or load the cosmic_works database\n",
    "database_name = \"cosmic_works\"\n",
    "db = None\n",
    "databases = list(client.list_databases())\n",
    "# Check if the database already exists\n",
    "for db_info in databases:\n",
    "    if db_info['id'] == database_name:\n",
    "        db = client.get_database_client(database_name)\n",
    "        print(f\"Database '{database_name}' already exists and has been retrieved.\")\n",
    "        break\n",
    "\n",
    "# Create the database if it does not exist\n",
    "if not db:\n",
    "    db: DatabaseProxy = client.create_database(database_name)\n",
    "    print(f\"Database '{database_name}' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Azure OpenAI connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_client = AzureOpenAI(\n",
    "    azure_endpoint = AOAI_ENDPOINT,\n",
    "    api_version = AOAI_API_VERSION,\n",
    "    api_key = AOAI_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize and store the embeddings in each document\n",
    "\n",
    "The process of creating a vector embedding field on each document only needs to be done once. However, if a document changes, the vector embedding field will need to be updated with an updated vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n",
    "def generate_embeddings(text: str):\n",
    "    '''\n",
    "    Generate embeddings from string of text using the deployed Azure OpenAI API embeddings model.\n",
    "    This will be used to vectorize document data and incoming user messages for a similarity search with\n",
    "    the vector index.\n",
    "    '''\n",
    "    response = ai_client.embeddings.create(input=text, model=EMBEDDINGS_DEPLOYMENT_NAME)\n",
    "    embeddings = response.data[0].embedding\n",
    "    time.sleep(0.5) # rest period to avoid rate limiting on AOAI\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate embeddings generation using a test string\n",
    "test = \"hello, world\"\n",
    "print(generate_embeddings(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize and update all product documents in the Cosmic Works database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector embedding policy\n",
    "vector_embedding_policy = {\n",
    "    \"vectorEmbeddings\": [\n",
    "        {\n",
    "            \"path\": \"/contentVector\",\n",
    "            \"dataType\": \"float32\",\n",
    "            \"distanceFunction\": \"cosine\",\n",
    "            \"dimensions\": 1536\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the indexing policy\n",
    "indexing_policy = {\n",
    "    \"indexingMode\": \"consistent\",  \n",
    "    \"automatic\": True, \n",
    "    \"includedPaths\": [\n",
    "        {\n",
    "            \"path\": \"/*\" \n",
    "        }\n",
    "    ],\n",
    "    \"excludedPaths\": [\n",
    "        {\n",
    "            \"path\": \"/\\\"_etag\\\"/?\"\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"/contentVector/*\"\n",
    "        }\n",
    "    ],\n",
    "    \"vectorIndexes\": [\n",
    "        {\n",
    "            \"path\": \"/contentVector\",\n",
    "            \"type\": \"diskANN\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "product_v_container = db.create_container_if_not_exists(\n",
    "    id=\"product_v\",\n",
    "    partition_key=PartitionKey(path=\"/categoryId\"),\n",
    "    indexing_policy=indexing_policy,\n",
    "    vector_embedding_policy=vector_embedding_policy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector embeddings for all products in the database\n",
    "product_container: ContainerProxy = db.create_container_if_not_exists(\n",
    "           id=\"product\",\n",
    "           partition_key={\"paths\": [\"/categoryId\"], \"kind\": \"Hash\"}\n",
    "       )\n",
    "\n",
    "T = TypeVar('T', bound=BaseModel)\n",
    "# Create generic helper function to query items a container.\n",
    "# This function re-uses the TypeVar and BaseModel from the Read a document example.\n",
    "def query_items(container, query, model: Type[T]) -> List[T]:\n",
    "    query = query\n",
    "    items = container.query_items(query=query, enable_cross_partition_query=True)\n",
    "    return [model(**item) for item in items]\n",
    "\n",
    "# retrieve all products via a query\n",
    "retrieved_products = query_items(product_container,\"SELECT * FROM prod\", Product)\n",
    "print(f\"Retrieved {len(retrieved_products)} products from the database.\")\n",
    "\n",
    "print(\"Starting the embedding of each product, this will take 3-5 minutes...\")\n",
    "# Populate contentVector field for each product in the product_v container that has vector indexing enabled\n",
    "for product in retrieved_products:\n",
    "    product.content_vector = generate_embeddings(product.model_dump_json(by_alias=True))    \n",
    "    product_v_container.upsert_item(product.model_dump(by_alias=True))\n",
    "\n",
    "print(\"Embedding complete and product_v container items updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use vector search in Azure Cosmos DB for NoSQL\n",
    "\n",
    "Now that each document has its associated vector embedding and the vector indexes have been created on each container, we can now use the vector search capabilities of Azure Cosmos DB for NoSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(\n",
    "        container: ContainerProxy, \n",
    "        prompt: str,         \n",
    "        vector_field_name:str=\"contentVector\", \n",
    "        num_results:int=5):\n",
    "    query_embedding = generate_embeddings(prompt)    \n",
    "    items = container.query_items(\n",
    "        query=f\"\"\"SELECT TOP @num_results itm.id, VectorDistance(itm.{vector_field_name}, @embedding) AS SimilarityScore \n",
    "                FROM itm\n",
    "                ORDER BY VectorDistance(itm.{vector_field_name}, @embedding)\n",
    "                \"\"\",\n",
    "        parameters = [\n",
    "            { \"name\": \"@num_results\", \"value\": num_results },\n",
    "            { \"name\": \"@embedding\", \"value\": query_embedding }            \n",
    "        ],\n",
    "        enable_cross_partition_query=True\n",
    "        )\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What bikes do you have?\"\n",
    "results = vector_search(product_v_container, prompt)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What do you have that is yellow?\"\n",
    "results = vector_search(product_v_container, prompt, num_results=4)\n",
    "for result in results:\n",
    "    print(result)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use vector search results in a RAG pattern with Chat GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generic function to query an item by its ID\n",
    "def query_item_by_id(container, id, model: Type[T]) -> T:\n",
    "    query = \"SELECT * FROM itm WHERE itm.id = @id\"\n",
    "    parameters = [\n",
    "        {\"name\": \"@id\", \"value\": id}\n",
    "    ]    \n",
    "    item = list(container.query_items(\n",
    "        query=query,\n",
    "        parameters=parameters,\n",
    "        enable_cross_partition_query=True\n",
    "    ))[0]\n",
    "    return model(**item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A system prompt describes the responsibilities, instructions, and persona of the AI.\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful, fun and friendly sales assistant for Cosmic Works, a bicycle and bicycle accessories store. \n",
    "Your name is Cosmo.\n",
    "You are designed to answer questions about the products that Cosmic Works sells.\n",
    "\n",
    "Only answer questions related to the information provided in the list of products below that are represented\n",
    "in JSON format.\n",
    "\n",
    "If you are asked a question that is not in the list, respond with \"I don't know.\"\n",
    "\n",
    "List of products:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_vector_search(\n",
    "        container: ContainerProxy, \n",
    "        prompt: str,         \n",
    "        vector_field_name:str=\"contentVector\", \n",
    "        num_results:int=5):\n",
    "    \"\"\"\n",
    "    Use the RAG model to generate a prompt using vector search results based on the\n",
    "    incoming question.  \n",
    "    \"\"\"\n",
    "    # perform the vector search and build product list\n",
    "    results = vector_search(container, prompt, vector_field_name, num_results)\n",
    "    product_list = \"\"\n",
    "    for result in results:\n",
    "        # retrieve the product details\n",
    "        product = query_item_by_id(container, result[\"id\"], Product)               \n",
    "        # remove the contentVector field from the product details, this isn't needed for the context\n",
    "        product.content_vector = None        \n",
    "        product_list += json.dumps(product, indent=4, default=str) + \"\\n\\n\"\n",
    "\n",
    "    # generate prompt for the LLM with vector results\n",
    "    formatted_prompt = system_prompt + product_list\n",
    "\n",
    "    # prepare the LLM request\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": formatted_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    completion = ai_client.chat.completions.create(messages=messages, model=COMPLETIONS_DEPLOYMENT_NAME)\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_with_vector_search(product_v_container, \"What bikes do you have?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_with_vector_search(product_v_container, \"What are the names and skus of yellow products?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
